---
layout: archive
title: " Chapter.9 ì •ì±…ê²½ì‚¬ë²•"
permalink: /coding/deeplearning/9/ 
author_profile: true
math: true
---

## ì •ì±…ê²½ì‚¬ë²•

Index

1. ì •ì±…ê²½ì‚¬ë²•
    - ê°€ì¹˜ê¸°ë°˜ê¸°ë²•
    - ì •ì±…ê¸°ë°˜ê¸°ë²• ë„ì¶œ
    - ì •ì±…ê²½ì‚¬ë²• ì•Œê³ ë¦¬ì¦˜
2. Reinforce
3. Baseline
    - ì•„ì´ë””ì–´
    - ì ìš©
4. í–‰ìœ„ì - ë¹„í‰ì
5. ì •ë¦¬

---  

ë°´ë””íŠ¸, ë²¨ë§Œ, DP, MC, TD   
Q-learning, DQN  
            ì •ì±…ê²½ì‚¬ë²•

---  

ê°•í™”í•™ìŠµì—ì„œ ì •ì±…ì„ ì–»ëŠ” ë°©ë²•ì€ Qí•¨ìˆ˜ê°’ìœ¼ë¡œ ì •ì±…ì„ ê²°ì •í•˜ëŠ” QëŸ¬ë‹ ê³„ì—´ê³¼ ì •ì±… ê·¸ ìì²´ë¥¼ í‰ê°€í•˜ëŠ” ë°©ë²• ë‘ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤. 

### ì •ì±…ê²½ì‚¬ë²• Policy Gradient Method

##### â‘´ ê°€ì¹˜ ê¸°ë°˜ ê¸°ë²• 
: ê°€ì¹˜í•¨ìˆ˜(V / Q)ë¥¼ í•™ìŠµí•˜ê³  ì´ë¥¼ ì •ì±…ì˜ í‰ê°€ì™€ ê°œì„ ì„ ë°˜ë³µí•˜ì—¬ ìµœì ì˜ í–‰ë™ì„ ì°¾ì•„ ìµœì  ì •ì±…ì„ ì°¾ì•„ê°„ë‹¤. ì¦‰, **ê°€ì¹˜í•¨ìˆ˜ ê²½ìœ ** í•˜ì—¬ ì •ì±…ì„ ì–»ëŠ”ë‹¤.  

- ê°€ì¹˜ ê¸°ë°˜ ê¸°ë²•ì˜ í•œê³„ 
1. Qê°’ì„ í†µí•´ ìµœì ì˜ í–‰ë™ì„ ì„ íƒí•˜ê¸° ë•Œë¬¸ì— ì •ì±…ì„ ì§ì ‘ ì¡°ì ˆí•˜ì§€ ëª»í•œë‹¤
2. í–‰ë™ì´ ì—°ì†ì ì´ë©´ Qí•¨ìˆ˜ë¡œ í‘œí˜„ì´ ì–´ë µë‹¤ 
3. epsilon-greedy ëŠ” í™•ë¥ ì  ì •ì±… í•™ìŠµì´ ì–´ë µë‹¤

    Ï€(s) = argmax_a Q(s, a)

    argmaxë¡œ ì œì¼ ì¢‹ì€ í–‰ë™ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” ê²°ì •ì  ì •ì±…ì„ ë”°ë¥´ëŠ”ë°, ì—¬ëŸ¬ í–‰ë™ ì¤‘ì—ì„œ í™•ë¥ ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ _í™•ë¥ ì  ì •ì±…_

    4. Qí•¨ìˆ˜ì˜ ì¶”ì •ì´ ì˜ëª»ëœ ê²½ìš° ì •ì±… ë˜í•œ ë¶ˆì•ˆì •í•´ì§„ë‹¤

##### â‘µ ì •ì±…ê²½ì‚¬ë²•
ê²½ì‚¬ë¥¼ ì´ìš©í•˜ì—¬ ì •ì±…ì„ ê°±ì‹ í•˜ëŠ” ê¸°ë²•ì˜ ì´ì¹­  
ì •ì±…ì„ ì§ì ‘ í•™ìŠµ, ì¡°ì •í•´ì„œ ê¸°ëŒ€ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ê¸°ë²•

- íŠ¹ì§•

1. ì •ì±…ì„ ì§ì ‘ í•™ìŠµí•œë‹¤
2. **ì—°ì†ì ì¸ í–‰ë™ í•™ìŠµ ê°€ëŠ¥** (ex. ê°€ì†ì„ 'ì–¼ë§ˆë‚˜' ë°Ÿì„ ê²ƒì¸ì§€)
3. í™•ë¥ ì •ì±… ê°€ëŠ¥
4. ë¯¸ë¶„ ê°€ëŠ¥í•œ ì •ì±…ì„ ê²½ì‚¬ë¡œ í•™ìŠµ ê°€ëŠ¥  

ì´ì‚°ì  í–‰ë™ - ê°€ì¹˜ê¸°ë°˜ê¸°ë²•  
ì—°ì†ì  í–‰ë™ - ì •ì±…ê²½ì‚¬ë²•  
 
---

ì •ì±…ê²½ì‚¬ë²•ì˜ ëª©ì  : ì¢‹ì€ ì •ì±…ì„ í•™ìŠµí•´ì„œ ì—ì´ì „íŠ¸ì˜ ë³´ìƒì„ ìµœëŒ€í™”í•˜ê¸°

í™•ë¥ ì  ì •ì±…(í•™ìŠµ ëŒ€ìƒ) :
$$
\pi(a|s)
$$  

ì •ì±…ì„ í•¨ìˆ˜ë¡œ í‘œí˜„í•˜ê³  ë¶™ì´ëŠ” ë³€ìˆ˜ : 
$$
\theta
$$  

<small>
-ì •ì±…ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„  
-ë³´í†µ ì‹ ê²½ë§ìœ¼ë¡œ $\theta$ í‘œí˜„ _ì‹ ê²½ë§ì„ ë„êµ¬ë¡œ_  
-ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ W ë§¤ê°œë³€ìˆ˜ í†µì¹­  
<small>

ì‹ ê²½ë§ìœ¼ë¡œ êµ¬í˜„í•œ ì •ì±… :
$$
\pi_\theta(a|s)
$$  

ëª©ì í•¨ìˆ˜ J (objective function) :

$$
\tau
$$ 
ëŠ” ì •ì±… 
$$
\pi_\theta(a|s)
$$ ë¥¼ ë”°ë¥¼ë•Œ, í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš©ìœ¼ë¡œ ìƒì„±ëœ í•˜ë‚˜ì˜ ê¶¤ì  

$$
\tau = (S_0, A_0, R_0, S_1, A_1, R_1, \dots, S_T)
$$

â†’ ì •ì±… 
$$
\pi(a|s)
$$ ì— ë”°ë¥¸ í–‰ë™ì˜ ê²°ê³¼ë¡œ ì–»ì€ ì‹œê³„ì—´ ë°ì´í„°   

$$
G(\tau) = R_0 + \gamma R_1 + \gamma^2 R_2 + \cdots + \gamma^T R_T
$$

â†’  ì •ì±…ë„ í™˜ê²½ë„ í™•ë¥ ì ì´ê¸° ë•Œë¬¸ì— ìˆ˜ìµë„ ë§¤ë²ˆ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.  
âˆ´ ìˆ˜ìµìœ¼ë¡œ ì •ì±…ì´ ì¢‹ì€ì§€ë¥¼ íŒë‹¨í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— **ìˆ˜ìµì˜ ê¸°ëŒ“ê°’ì´ ëª©ì í•¨ìˆ˜**

- ì •ì±…ìœ¼ë¡œ ëª©ì í•¨ìˆ˜ ì„¤ì • :  
ì •ì±…ì„ ì‚¬ìš©í•´ì„œ ì—¬ëŸ¬ë²ˆ ì‹œë„í–ˆì„ë•Œ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë³´ìƒì„ ë°›ëŠ”ì§€
**ì •ì±… ê²½ì‚¬ë²•ì—ì„œ ëª©ì í•¨ìˆ˜J ëŠ” ê²½ì‚¬ìƒìŠ¹ë²•ìœ¼ë¡œ ìµœëŒ“ê°’ì„ ì°¾ëŠ”ë‹¤.**  


$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[G(\tau)]
$$


- ì •ì±… ê°±ì‹ ë²•

$$
Î¸ â† Î¸+Î±âˆ‡_Î¸J(Î¸)
$$  


---


- ì†ì‹¤í•¨ìˆ˜ì™€ ëª©ì í•¨ìˆ˜ì˜ ì°¨ì´ì 

| í•­ëª©             | DQN (ì†ì‹¤í•¨ìˆ˜)                           | ì •ì±…ê²½ì‚¬ë²• (ëª©ì í•¨ìˆ˜)                    |
|------------------|------------------------------------------|------------------------------------------|
| í•™ìŠµ ê¸°ì¤€        | ì˜¤ì°¨(ì˜ˆì¸¡ vs ì‹¤ì œ)                      | ê¸°ëŒ€ ë³´ìƒ                               |
| í•¨ìˆ˜ ì´ë¦„        | Loss $$L(\theta)$$                      | Objective $$J(\theta)$$                 |
| ìˆ˜ì‹ ì˜ˆ          | $$L(\theta) = (y - Q(s, a; \theta))^2$$ | $$J(\theta) = \mathbb{E}[G(\tau)]$$     |
| ìµœì í™” ë°©í–¥      | ê²½ì‚¬ í•˜ê°• â†“                              | ê²½ì‚¬ ìƒìŠ¹ â†‘                              |
| ì—­í•              | Q ì¶”ì • ì •í™•ë„ â†‘                          | ì •ì±… ì„±ëŠ¥ â†‘                              |

ì„ í˜•íšŒê·€ëŠ” ì†ì‹¤í•¨ìˆ˜L ì€ Qê°’ì˜ ì˜ˆì¸¡ì´ ì‹¤ì œê°’ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ë¥¼ êµ¬í•˜ê³  ì´ë¥¼ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì†Œí™” 

---

[ë„ì¶œ ê³¼ì •]  

â‘  ëª©ì í•¨ìˆ˜

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[G(\tau)]
$$

â‘¡ ê¸°ëŒ“ê°’ì˜ ë¯¸ë¶„ ê³„ì‚° - ê¸°ìš¸ê¸° êµ¬í•˜ê¸°  

$$
\nabla_\theta J(\theta) = \nabla_\theta \mathbb{E}_{\tau}[G(\tau)]
$$  

<small> 
- ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ì´ìœ    
:ê¸°ìš¸ê¸°ëŠ” ìµœëŒ€ ì¦ê°€ ë°©í–¥ì„ ì•Œë ¤ì£¼ëŠ” ë²¡í„°ê°’ì´ê¸° ë•Œë¬¸
<small> 

â‘¢ ì‹¤ì œ ê³„ì‚°

$$
\nabla_\theta J(\theta)
= \mathbb{E}_{\tau} \left[ \sum_{t=0}^T G(\tau) \nabla_\theta \log \pi_\theta(A_t | S_t) \right]
$$

ê¶¤ì ì„ ì—¬ëŸ¬ë²ˆ ìƒ˜í”Œë§í•´ì„œ í‰ê· ì„ ë‚´ëŠ” ê²ƒ (ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ ê¸°ë²•) ìœ¼ë¡œ ì •ë¦¬  
**ëª©ì í•¨ìˆ˜ëŠ” ìˆ˜ìµê°’ x ì •ì±…ì„ ë¯¸ë¶„í•œ ê°’ì˜ í‰ê· **  

- í™•ë¥  ê·¸ ìì²´ë³´ë‹¤ ë¡œê·¸í™•ë¥ ì˜ ë¯¸ë¶„ì´ ë” ì•ˆì •ì ì´ê¸° ë•Œë¬¸ì— ì •ì±…ì— ë¡œê·¸ë¥¼ ì·¨í•œë‹¤  
- ì •ì±… $$ 
\pi_\theta(a|s) 
$$ ë¥¼ Î¸ë¡œ ë¯¸ë¶„í•˜ê²Œ ë˜ë©´ ê²½ì‚¬ê°€ êµ¬í•´ì§„ë‹¤. 
- $$
\nabla_\theta \log \pi_\theta(a|s)
$$ ëŠ” ì •ì±…ì´ ê·¸ í–‰ë™ì„ ì„ íƒí•˜ë„ë¡ ë§Œë“  ê¸°ì—¬ë„ì˜ ë²¡í„°ê°’ì„ ëœ»í•œë‹¤. 
- ìˆ˜ìµê°’ì€ ê°€ì¤‘ì¹˜ì˜ ì—­í• ì„ í•œë‹¤ == ë³´ìƒì„ ë§ì´ ë°›ì€ í–‰ë™ì˜ í™•ë¥ ì´ ë†’ì¸ë‹¤.  

---

##### â‘¶ ì •ì±…ê²½ì‚¬ë²• ì•Œê³ ë¦¬ì¦˜ 

- êµ¬í˜„ ì½”ë“œ 1. ì‹ ê²½ë§ ìƒì„± (í™•ë¥ ì  ì •ì±…í•¨ìˆ˜ ì •ì˜)

```python
if '__file__' in globals():
    import os, sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
import numpy as np
import gym
from dezero import Model
from dezero import optimizers
import dezero.functions as F
import dezero.layers as L


class Policy(Model):
    def __init__(self, action_size):
        super().__init__()
        self.l1 = L.Linear(128)          # ì²« ë²ˆì§¸ ê³„ì¸µ
        self.l2 = L.Linear(action_size)  # ë‘ ë²ˆì§¸ ê³„ì¸µ

    def forward(self, x):
        x = F.relu(self.l1(x))     # ì²« ë²ˆì§¸ ê³„ì¸µì—ì„œëŠ” ReLU í•¨ìˆ˜ ì‚¬ìš©
        x = F.softmax(self.l2(x))  # ë‘ ë²ˆì§¸ ê³„ì¸µì—ì„œëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ ì‚¬ìš©
        return x
```
- ë³µì¡í•œ í™˜ê²½ì—ì„œ í™•ë¥ ì ì¸ ì •ì±…ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ ì‹ ê²½ë§ ì‚¬ìš©
- ì´ë•Œ ì™„ì „ì—°ê²°ì¸µì´ 1ì¸µì´ë©´ í‘œí˜„ë ¥ì´ ì•½í•˜ê³  3ì¸µì´ìƒì´ë©´ ê³¼ì í•© ìœ„í—˜ì´ ìˆê¸° ë•Œë¬¸ì— ì•ˆì •ì ì¸ 2ì¸µì˜ ì™„ì „ ì—°ê²° ëª¨ë¸ êµ¬í˜„ (ì¹´íŠ¸í´ êµ¬í˜„ì„ ìœ„í•œ) 
- ReLUë¡œ ë¹„ì„ í˜•ì„±ì„ ë„ì…   
- Softmax í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ê°œì˜ ê°’ì„ ë°›ê³  ì „ì²´í•©ì´ 1ì´ ë˜ëŠ” í™•ë¥ ë¶„í¬ë¡œ ë³€í™˜í•˜ëŠ” ì—­í•   
ex) x = [2.1, 1.0] â†’ softmax(x) = [0.75, 0.25]

```
ì…ë ¥: state (ì˜ˆ: CartPoleì—ì„œëŠ” 4ì°¨ì› ë²¡í„°)
â†“
1ì¸µ: Linear(4 â†’ 128), ReLU
â†“
2ì¸µ: Linear(128 â†’ í–‰ë™ ìˆ˜), Softmax
â†“
ì¶œë ¥: [0.75, 0.25] â†’ í™•ë¥  ë¶„í¬ (ì •ì±… Ï€Î¸(a|s))
```

- êµ¬í˜„ì½”ë“œ 2. ì—ì´ì „íŠ¸ í–‰ë™ & í•™ìŠµ

```python
class Agent:
    def __init__(self):
        self.gamma = 0.98
        self.lr = 0.0002
        self.action_size = 2

        self.memory = [] #ì—í”¼ì†Œë“œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë©”ëª¨ë¦¬
        self.pi = Policy(self.action_size) # ì •ì±… ì‹ ê²½ë§ ìƒì„±
        self.optimizer = optimizers.Adam(self.lr) # ì˜µí‹°ë§ˆì´ì € ì„¤ì • -> íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜
        self.optimizer.setup(self.pi) # ì •ì±… íŒŒë¼ë¯¸í„° ë“±ë¡ 

    def get_action(self, state):
        state = state[np.newaxis, :]  # ì°¨ì› ë§ì¶”ê¸°
        probs = self.pi(state)        # ìˆœì „íŒŒ ìˆ˜í–‰, ì •ì±…ì‹ ê²½ë§ì— ìƒíƒœë¥¼ ì…ë ¥í•˜ì—¬ í™•ë¥ ì„ ì¶œë ¥
        probs = probs[0] # í™•ë¥ ì„ 1ì°¨ì›ìœ¼ë¡œ ì •ë¦¬
        action = np.random.choice(len(probs), p=probs.data)  # í™•ë¥ ì  í–‰ë™ ì„ íƒ
        return action, probs[action]  # ì„ íƒëœ í–‰ë™ê³¼ í™•ë¥  ë°˜í™˜

    def add(self, reward, prob):
        data = (reward, prob)
        self.memory.append(data) #ì •ì±… ì—…ë°ì´íŠ¸ì‹œ í•„ìš”í•œ ë³´ìƒê³¼ í–‰ë™í™•ë¥  ì €ì¥

    def update(self):
        self.pi.cleargrads()

        G, loss = 0, 0
        for reward, prob in reversed(self.memory):  # ìˆ˜ìµ G ê³„ì‚°
            G = reward + self.gamma * G

        for reward, prob in self.memory:  # ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°
            loss += -F.log(prob) * G

        loss.backward()
        self.optimizer.update()
        self.memory = []  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
```

- ëª©ì í•¨ìˆ˜ê°€ ì•„ë‹Œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì“°ëŠ” ì´ìœ   

ë”¥ëŸ¬ë‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ backward(), step() ë“±ì˜ ë©”ì†Œë“œê°€ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ì´ë¯¸ ë§Œë“¤ì–´ì ¸ìˆë‹¤.  

ëª©ì í•¨ìˆ˜ëŠ” ìŠ¤ì¹¼ë¼ê°’ì´ë©° -ì†ì‹¤í•¨ìˆ˜

| Î¸    | LÎ¸ | -LÎ¸ |
|-------------|-------------|--------------|
| Î¸_1 | 10          | -10          |
| Î¸_2 | 2           | -2           |

ë”°ë¼ì„œ **'-ì†ì‹¤í•¨ìˆ˜'ë¥¼ ìµœì†Œí™”í•˜ë©´ ëª©ì í•¨ìˆ˜ê°€ ìµœëŒ€í™”ëœë‹¤**
loss += -F.log(prob) * G  


- êµ¬í˜„ì½”ë“œ 3. ì¹´íŠ¸í´ ì‹¤í–‰

```python
episodes = 3000
env = gym.make('CartPole-v0', render_mode='rgb_array')
agent = Agent()
reward_history = []

for episode in range(episodes):
    state = env.reset()[0]
    done = False
    total_reward = 0

    while not done:
        action, prob = agent.get_action(state)  # í–‰ë™ ì„ íƒ
        next_state, reward, terminated, truncated, info = env.step(action)  # í–‰ë™ ìˆ˜í–‰
        done = terminated | truncated

        agent.add(reward, prob)  # ë³´ìƒê³¼ í–‰ë™ì˜ í™•ë¥ ì„ ì—ì´ì „íŠ¸ì— ì¶”ê°€
        state = next_state       # ìƒíƒœ ì „ì´
        total_reward += reward   # ë³´ìƒ ì´í•© ê³„ì‚°

    agent.update()  # ì •ì±… ê°±ì‹ 

    reward_history.append(total_reward)
    if episode % 100 == 0:
        print("episode :{}, total reward : {:.1f}".format(episode, total_reward))

from common.utils import plot_total_reward
plot_total_reward(reward_history)
```

- ì—í”¼ì†Œë“œë³„ ë³´ìƒ í•©ê³„ ì¶”ì´

<div style="display: flex; gap: 10px; margin-bottom: 30px;">
  <img src="/assets/images/9_1.png" alt="1" width="500">
  <img src="/assets/images/9_2.png" alt="2" width="500">
</div>

--- 

[ì •ì±…ê²½ì‚¬ë²• (Policy Gradient)]  
â”œâ”€â”€ REINFORCE (Monte Carlo ê¸°ë°˜)  
â”œâ”€â”€ Actor-Critic  
â”‚   â”œâ”€â”€ Advantage Actor-Critic (A2C)  
â”‚   â””â”€â”€ A3C, PPO ë“±...  


### 2. REINFORCE ì•Œê³ ë¦¬ì¦˜

ì•ì„  ì •ì±…ê²½ì‚¬ë²•ì˜ ì½”ë“œì—ì„œëŠ” ê°€ì¤‘ì¹˜ Gë¥¼ ëª¨ë“  ë³´ìƒì„ ë”í•œ ê°’ìœ¼ë¡œ ê³„ì‚°í–ˆì§€ë§Œ, íŠ¹ì •ì‹œê°„ tì—ì„œ í–‰ë™ Aì´ ì¢‹ì€ì§€ ë‚˜ìœì§€ëŠ” í–‰ë™ Aë¥¼ í•˜ê³  ë‚œ í›„ì˜ ë³´ìƒì˜ ì´í•©ìœ¼ë¡œ í‰ê°€ë˜ê¸° ë•Œë¬¸ì— í–‰ë™ ì „ì— ì–»ì€ ë³´ìƒì€ ê´€ë ¨ì´ ì—†ëŠ” ë…¸ì´ì¦ˆê°€ ëœë‹¤. REINFORCE ì•Œê³ ë¦¬ì¦˜ì€ ì´ë¥¼ ê°œì„ í•œ ê¸°ë²•    
( **RE**ward **I**ncrement = **N**onnegative **F**ator X **O**ffset **R**einforcement X **C**haracteristic **E**ligibility )

$$
G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots + \gamma^{T - t} R_T
$$

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} G_t \nabla_\theta \log \pi_\theta (A_t \mid S_t) \right]
$$


```python
# ì •ì±…ê²½ì‚¬ë²•

    def update(self):
        self.pi.cleargrads()

        G, loss = 0, 0
        for reward, prob in reversed(self.memory):  # ìˆ˜ìµ G ê³„ì‚°
            G = reward + self.gamma * G

        for reward, prob in self.memory:  # ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°
            loss += -F.log(prob) * G

        loss.backward()
        self.optimizer.update()
        self.memory = []  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”

```

```python
# REINFORCE ê¸°ë²•
    def update(self):
        self.pi.cleargrads()

        G, loss = 0, 0
        for reward, prob in reversed(self.memory):
            G = reward + self.gamma * G  # ìˆ˜ìµ G ê³„ì‚°
            loss += -F.log(prob) * G     # ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°

        loss.backward()
        self.optimizer.update()
        self.memory = []

```
- ì—í”¼ì†Œë“œë³„ ë³´ìƒ í•©ê³„ ì¶”ì´

<div style="display: flex; gap: 10px; margin-bottom: 30px;">
  <img src="/assets/images/9_3.png" alt="1" width="500">
  <img src="/assets/images/9_4.png" alt="2" width="500">
</div>

---

### 3. BASELINE ì•Œê³ ë¦¬ì¦˜

REINFORCE ì•Œê³ ë¦¬ì¦˜ ê°œì„  _ ë¶„ì‚°ì„ ì¤„ì´ì  

â‘´ ì•„ì´ë””ì–´

í˜„ì¬ì˜ ê²°ê³¼ë§Œìœ¼ë¡œ ë¹„êµë¥¼ í•˜ë©´ ë¶„ì‚°ì´ ì»¤ì§€ëŠ” ë¬¸ì œì ì´ ìˆë‹¤.  
ì‹¤ì œ ê²°ê³¼ - ê³¼ê±°ì˜ í‰ê· (ì˜ˆì¸¡ê°’)ìœ¼ë¡œ ë„ì¶œë˜ëŠ” ì°¨ì´ë¡œ ë¹„êµë¥¼ í•˜ê²Œ ë˜ë©´ ë¶„ì‚°ì´ ì‘ì•„ì§„ë‹¤. 

â‘µ ì ìš©  

REINFORCEëŠ” G_tê°€ í´ìˆ˜ë¡ ì •ì±…ì„ ê°•í™”í•˜ê¸°ë•Œë¬¸ì— ëª¨ë“  í–‰ë™ì´ ê°•í™”ëœë‹¤. 

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} G_t \nabla_\theta \log \pi_\theta(A_t | S_t) \right]
$$


í•˜ì§€ë§Œ **BASELINEì€ G_tê°€ í‰ê· ë³´ë‹¤ ë†’ìœ¼ë©´ ì •ì±…ì„ ê°•í™”í•˜ê³  ë‚®ìœ¼ë©´ ì•½í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¶„ì‚°ì„ ì¤„ì—¬, í•™ìŠµì„ ì•ˆì •í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤.  

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} (G_t - b(S_t)) \nabla_\theta \log \pi_\theta(A_t | S_t) \right]
$$

b(S_t) ë¥¼ ë² ì´ìŠ¤ë¼ì¸ì´ë¼ê³  ë¶€ë¥´ë©° ì´ëŠ” ì„ì˜ì˜ í•¨ìˆ˜ë¡œ ë³´í†µ ê°€ì¹˜í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— b(S_t)= ğ‘‰ğœ‹(ğ‘†_ğ‘¡)  
í•˜ì§€ë§Œ ì •ì±…ê²½ì‚¬ë²•ì—ì„œëŠ” ì •ì±…ì´ ê³„ì† ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ì •í™•í•œ ê°€ì¹˜í•¨ìˆ˜ì˜ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì‹ ê²½ë§ì„ í†µí•´ ê·¼ì‚¬  
â†’ ë‹¤ì‹œ ê°€ì¹˜ ê¸°ë°˜ ê¸°ë²•(Value-based)ìœ¼ë¡œ ì ‘ê·¼  
â†’ ë² ì´ìŠ¤ë¼ì¸ì€ ê°€ì¹˜ê¸°ë°˜ê³¼ ì •ì±…ê¸°ë°˜ì˜ í˜¼í•©ëœ í˜•íƒœ  

### 4. í–‰ìœ„ìì™€ ë¹„í‰ì

ì§€ê¸ˆê¹Œì§€ì˜ ì •ì±…ê²½ì‚¬ë²• ì½”ë“œì—ì„œëŠ” MCë²•ì„ ì‚¬ìš©í•´ì„œ ì—í”¼ì†Œë“œê°€ ë‹¤ ëë‚œ í›„ì— ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í–ˆë‹¤. 
MCëŠ” ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ í•  ìˆ˜ ì—†ê³ , ë°ì´í„°ë¥¼ ë‹¤ ëª¨ì•„ì•¼ í•™ìŠµì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì‹œê°„ ì‹œìŠ¤í…œì´ë‚˜ ê³„ì†í•´ì„œ ì›€ì§ì´ëŠ” ë¡œë´‡ ë“±ì˜ í•™ìŠµì— í•œê³„ê°€ ìˆë‹¤.  
í•˜ì§€ë§Œ TDë²•ì„ ì‚¬ìš©í•˜ë©´ í•œìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆê²Œ ë˜ë©´ì„œ ë¬´í•œí•œ ì—í”¼ì†Œë“œ í™˜ê²½ì—ì„œë„ í•™ìŠµì´ ê°€ëŠ¥í•´ì§€ê³  ë¹ ë¥´ê³  ì•ˆì •ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.  

MCë²• ) ê°€ì¹˜í•¨ìˆ˜ V_t -> G_t  
TDë²• ) ê°€ì¹˜í•¨ìˆ˜ V_t -> R_t + Î³ * V_(t+1)  

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \left( R_t + \gamma V_w(S_{t+1}) - V_w(S_t) \right) \nabla_\theta \log \pi_\theta(A_t \mid S_t) \right]
$$

â‡’ TD-ê¸°ë°˜ í–‰ìœ„ì-ë¹„í‰ì(Actor-Critics)  

ì–´ë–¤ í–‰ë™ì„ í• ì§€ ê²°ì •í•˜ëŠ” ì •ì±…ì„ í–‰ìœ„ì Actorë¡œ ë³´ê³  ì´ ì •ì±…ì„ í‰ê°€í•˜ëŠ” ê°€ì¹˜í•¨ìˆ˜ë¥¼ ë¹„í‰ì Criticë¡œ ë‚˜ëˆ„ì–´ë³´ê¸° ë•Œë¬¸ì— 
ì´ë•Œ ì •ì±…ê³¼ ê°€ì¹˜í•¨ìˆ˜ëŠ” ëª¨ë‘ ì‹ ê²½ë§ìœ¼ë¡œ ë‘ ì‹ ê²½ë§ì„ ë³‘ë ¬ë¡œ í•™ìŠµì‹œì¼œ Vì˜ ê°’ì´ R_t + Î³ * V_(t+1) ì— ê·¼ì‚¬í•˜ë„ë¡ í•™ìŠµì‹œí‚¨ë‹¤  

- ì½”ë“œ êµ¬í˜„  

```python
if '__file__' in globals():
    import os, sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
import numpy as np
import gym
from dezero import Model
from dezero import optimizers
import dezero.functions as F
import dezero.layers as L


class PolicyNet(Model):  # ì •ì±… ì‹ ê²½ë§
    def __init__(self, action_size=2):
        super().__init__()
        self.l1 = L.Linear(128)
        self.l2 = L.Linear(action_size)

    def forward(self, x):
        x = F.relu(self.l1(x))
        x = self.l2(x)
        x = F.softmax(x)  # í™•ë¥  ì¶œë ¥
        return x


class ValueNet(Model):  # ê°€ì¹˜ í•¨ìˆ˜ ì‹ ê²½ë§
    def __init__(self):
        super().__init__()
        self.l1 = L.Linear(128)
        self.l2 = L.Linear(1)

    def forward(self, x):
        x = F.relu(self.l1(x))
        x = self.l2(x)
        return x

class Agent:
    def __init__(self):
        self.gamma = 0.98
        self.lr_pi = 0.0002
        self.lr_v = 0.0005
        self.action_size = 2

        self.pi = PolicyNet()
        self.v = ValueNet()
        self.optimizer_pi = optimizers.Adam(self.lr_pi).setup(self.pi)
        self.optimizer_v = optimizers.Adam(self.lr_v).setup(self.v)

    def get_action(self, state):
        state = state[np.newaxis, :]  # ë°°ì¹˜ ì²˜ë¦¬ìš© ì¶• ì¶”ê°€
        probs = self.pi(state)
        probs = probs[0]
        action = np.random.choice(len(probs), p=probs.data)
        return action, probs[action]  # ì„ íƒëœ í–‰ë™ê³¼ í•´ë‹¹ í–‰ë™ì˜ í™•ë¥  ë°˜í™˜

    def update(self, state, action_prob, reward, next_state, done):
        # ë°°ì¹˜ ì²˜ë¦¬ìš© ì¶• ì¶”ê°€
        state = state[np.newaxis, :]
        next_state = next_state[np.newaxis, :]

        # ê°€ì¹˜ í•¨ìˆ˜(self.v)ì˜ ì†ì‹¤ ê³„ì‚°
        target = reward + self.gamma * self.v(next_state) * (1 - done)  # TD ëª©í‘œ
        target.unchain()
        v = self.v(state)  # í˜„ì¬ ìƒíƒœì˜ ê°€ì¹˜ í•¨ìˆ˜
        loss_v = F.mean_squared_error(v, target)  # ë‘ ê°’ì˜ í‰ê·  ì œê³± ì˜¤ì°¨

        # ì •ì±…(self.pi)ì˜ ì†ì‹¤ ê³„ì‚°
        delta = target - v
        delta.unchain()
        loss_pi = -F.log(action_prob) * delta

        # ì‹ ê²½ë§ í•™ìŠµ
        self.v.cleargrads()
        self.pi.cleargrads()
        loss_v.backward()
        loss_pi.backward()
        self.optimizer_v.update()
        self.optimizer_pi.update()


episodes = 3000
env = gym.make('CartPole-v0', render_mode='rgb_array')
agent = Agent()
reward_history = []

for episode in range(episodes):
    state = env.reset()[0]
    done = False
    total_reward = 0

    while not done:
        action, prob = agent.get_action(state)
        next_state, reward, terminated, truncated, info = env.step(action)
        done = terminated | truncated

        agent.update(state, prob, reward, next_state, done)

        state = next_state
        total_reward += reward

    reward_history.append(total_reward)
    if episode % 100 == 0:
        print("episode :{}, total reward : {:.1f}".format(episode, total_reward))

from common.utils import plot_total_reward
plot_total_reward(reward_history)
```

### 5. ì •ë¦¬  

ì •ì±… ê²½ì‚¬ë²•ì€ ê°€ì¹˜ê¸°ë°˜ê¸°ë²•ì˜ í•œê³„ë¥¼ ë³´ì™„í•œ ê¸°ë²• 

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \Phi_t \nabla_\theta \log \pi_\theta(A_t | S_t) \right]
$$

Î¦_ğ‘¡â€‹ëŠ” ê° ê¸°ë²•ë§ˆë‹¤ ë‹¬ë¼ì§€ëŠ” ë³´ìƒì˜ ì²™ë„ì´ì ê¸°ì—¬ë„ ê³„ìˆ˜  

1. $$
\Phi_t = G(\tau) $$â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# ê°€ì¥ ë‹¨ìˆœí•œ ì •ì±… ê²½ì‚¬ë²•  

2. $$
\Phi_t = G_t$$â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# REINFORCE  

3. $$
\Phi_t = G_t - b(S_t) 
$$â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒ# ë² ì´ìŠ¤ë¼ì¸ì„ ì ìš©í•œ REINFORCE  

4. $$
\Phi_t = R_t + \gamma V(S_{t+1}) - V(S_t) 
$$â€ƒ# í–‰ìœ„ìâ€“ë¹„í‰ì (Actorâ€“Critic)  