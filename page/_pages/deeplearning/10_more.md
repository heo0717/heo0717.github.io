---
layout: archive
title: " Chapter.10"
permalink: /coding/deeplearning/more/ 
author_profile: true
math: true
---

심층강화학습의 알고리즘 소개  

INDEX  

1. 심층강화학습 알고리즘 분류  
2. 정책경사법 계열 고급 알고리즘
    - A3C , A2C
    - DDPG
    - TRPO, PRO
3. DQN 계열 고급 알고리즘
    - 범주형 DQN
    - Noisy Network
    - 레인보우 
4. 사례연구
    - 보드게임
    - 로봇 제어
    - NAS

5. 심층강화학습의 과제와 가능성 
6. MDP 공식화   

---

### 1. 심층강화학습 알고리즘 분류  

강화학습 + 딥러닝  

<div style="display: flex; gap: 10px; margin-bottom: 30px;">
  <img src="/assets/images/10_algorythm.png" alt="algorythm" width="800">
</div>

- 첫번째 기준 : 환경이 있는가  

환경모델 ( 상태전이함수와 보상함수 ) 사용 O → 모델기반 Model Based  
환경모델 ( 상태전이함수와 보상함수 ) 사용 X → 모델프리 Model Free  

- 두번째 기준 :  

모델 기반)  
모델 이용 (행동 X)  -> 알파고, 알파제로
모델 학습 (행동 O)  -> World Models, MBVE   

모델 프리)  
가치 기반 기법  -> DQN
정책 기반 기법  -> REINFORCE 

---

### 2. 정책경사법 계열 고급 알고리즘

⑴ 정책 경사법 계열 고급 알고리즘  

- A3C ( Asynchronous Advanced Actor-Critics )  (316p)

[AAA]  
A synchronous : 비동기 | 작업을 병렬로 처리 가능  
A dvantage : Q함수와 V함수의 차이 | action이 얼마나 좋은가  
A ctor : Actor - Critic | 정책을 배우고 그 정책을 평가  

<div style="display: flex; gap: 10px; margin-bottom: 30px;">
  <img src="/assets/images/A3C.png" alt="A3C" width="800">
</div>

**여러 에이전트가 각 환경(지역신경망)에서 병렬로 행동하고, 전역 신경망으로 정책의 기울기(매개변수)를 갱신한다.**

여러 에이전트가 있기 때문에 학습 속도가 높아지고 각 에이전트가 독립적인 환경에서 행동하기 때문에 다양한 데이터를 얻을 수 있다.  

⇒ 경험재생기법이 오프-정책에서만 활용될 수 있는 한계를 극복한 대안이 될 수 있다. **온-정책에서도 사용될 수 있는 아이디어로, 데이터의 상관관계를 약화시킬 수 있다.**

전역 신경망의 가중치 매개변수를 갱신하는 동안 주기적으로 전역 & 지역 신경망의 가중치 매개변수를 동기화한다.  

⇒ 하나의 신경망에서 계산을 하다가 끝부분만 ReLU와 Softmax로 나뉘게 되는 신경망 구조를 가지고 있기 때문에 정책과 가치함수가 가중치를 공유한다고 이야기한다. 따라서 **계산에 효율성이 증가하고, 여러 에이전트가 같은 기준으로 연산을 처리하기 때문에 안정적**

- A2C

[AA]
A dvantage : Q함수와 V함수의 차이 | action이 얼마나 좋은가  
A ctor : Actor - Critic | 정책을 배우고 그 정책을 평가  

A2C는 A3C에서 비동기가 빠진 형태이다. 즉 동기방식을 취하는데 이는 에이전트들이 시간 t 에서 각 환경의 상태를 동기화하여 배치로 묶고 신경망에 학습을 시키는 방식이다. 이때 신경망이 정책을 출력하고 이를 각 환경에 전달하는 방식이다. 
**모든 에이전트가 동시에 업데이트를 해야한다는 점에서 비동기와 차이가 있다.**  

⇒ 멀티스레딩이 필요없기때문에 구현이 쉽고 갱신 성능도 A3C와 비슷하기 때문에 실무에서는 A2C를 많이 사용  

---

- DDPG (320p)

DDPG ( Deep Deterministic Policy Gradient method / 심층 결정적 정책 경사법 ) 은 정책경사법의 장점인 **행동공간이 연속적인 문제에 맞춰 설계된 알고리즘**이다.

연속적인 환경 ex) 미세한 조작이 필요한 로봇의 팔 등  

앞선 A3C, A2C는 행동을 정책의 확률분포에서 무작위로 샘플링하기때문에 탐색을 하는 과정을 거치지만 DDPG는 행동의 정해진 값을 그대로 출력해서 실행한다. 
즉, **DDPG 정책에서 특정 상태 s를 입력했을때 행동 a가 고유하게 결정되는 결정적 정책이다. 또한, 행동이 실수인 경우에도 학습이 가능하다.** 

ex) 상태 s에서는 로봇의 팔을 1.6 만큼 회전한다. 

이산적인 행동에만 적용 가능했던 DQN의 한계  
정책기반기법에서 정책을 평가할 Q가 없다는 한계  

이를 극복하기 위해서 **연속된 공간에서 Q함수로 정책을 평가할 수 있도록 합친 것이 DDPG.**  
따라서 DDPG는 정책 신경망과 Q함수 신경망 두가지로 나뉘고 Q함수가 커지도록 정책 매개변수 θ 갱신 / DQN의 Q러닝으로 Q의 매개변수 Φ 갱신  

활용 예 ) 자율 주행 자동차의 차선 유지, 일정간격 유지 / 로봇이 컵을 부드럽게 잡도록 학습  

---

- TRPO  (321p)

정책경사법에서 사용하는 경사는 기대보상이 빠르게 증가하는 방향을 나타내는 벡터. 따라서 매개변수의 갱신 **방향**은 알 수 있으나 갱신의 **폭**은 알수 없다는 단점이 있다.  

이 변화량이 급격하거나 너무 없으면 정책이 나빠지기 때문에 **정책을 신뢰구간에서만 조금씩 바꿔서 정책이 갑자기 너무 많이 바뀌지 않도록 제약을 걸어주는것**이 TRPO (Truth Region Policy) / 신뢰영역 정책 최적화이다.  

이때 쿨백-라이블러 발산(KLD) 를 지표로 두 정책의 확률 분포의 유사도를 측정하고 임계값을 넘지않도록 제약을 부과하는 개념을 사용한다. 다만, 계산량이 많아지기 때문에 이 개념에서 KL 발산 제약이 아닌 비율 클리핑 방식을 활용해 계산을 간단하게 바꾼 것이 PPO이다.

활용 예 ) 소프트 로봇(공기압 기반 소프트그리퍼 등) 의 움직임 제어 

행동과 상태가 있는 모델기반의 제어가 아닌 데이터기반 + 강화학습 기반의 제어 
실수 값을 사용한 연속적인 움직임 제어가 필요하며 작은 움직임에도 민감하기 때문에 정책이 튀지않도록 관리  

<div style="display: flex; gap: 10px; margin-bottom: 30px;">
  <img src="/assets/images/soft_robot1.png" alt="soft robot" 
width="400">
<img src="/assets/images/soft_robot2.png" alt="soft robot" 
width="400">
</div> 

--- 

### 3. DQN 계열 고급 알고리즘

- DQN의 개념  

$$
Q_\pi(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]
$$

- 범주형 DQN  

범주형 DQN은 G의 기댓값이 아닌 확률 분포를 학습시키는 **분포 강화 학습**   
G의 확률 분포 : Z(s,a)  

$$
Q_\pi(s, a) = \mathbb{E} \left[ Z_\pi(s, a) \right]
$$

- Noisy Network  

지금까지 ε-greedy로 정책을 평가하고 개선  
이때 ε은 하이퍼파라미터로 적절하게 정해왔지만 이를 어떻게 설정하느냐에 따라 정확도가 달라진다. 따라서 **Noisy Network로 ε 설정 문제를 해결한다**  

- 레인보우 Rainbow  

다양한 DQN 확장 알고리즘은 개별적으로 성능을 향상하기 때문에 모두 결합한 것이 레인보우 기법은 성능이 아주 높게 나타나며, 완성판이라고 볼 수 있다.  

<div style="display: flex; gap: 10px; margin-bottom: 30px;">
  <img src="/assets/images/rainbow.png" alt="rainbow" 
width="800">
</div> 

- 레인보우 이후 알고리즘 _ 분산 병렬 학습

① Ape-X : 각 에이전트의 ε 을 다르게 설정하여 다양한 경험 데이터 수집  
② R2D2 : Ape-X 개선 + RNN으로 시계열 데이터 처리
③ NGU ( Never Give Up ) : R2D2 + 내적보상 추가로 보상이 적은 과제에서 탐색을 포기하지 않도록 보상의 크기가 아닌 호기심에 따라 행동하도록 유도  
④ Agent57 : NGU 발전 + 에이전트별 정책을 유연하게 분배  

---  

### 4. 사례연구  

딥러닝 사용 사례  

- 보드게임 

보드게임의 특성  

- 보드의 모든 정보를 알 수 있음 (완전 정보)  
- 제로썸 게임  
- 상태 전이가 결정적  

이러한 특성의 게임은 수를 읽고 예측하여 더 좋은 수를 두는 것이 중요하다. 이는 게임 트리로 표현할 수 있어, 가능한 수의 결과가 드러나기 때문에 최선의 수를 찾을 수 있게 된다.  
하지만 **바둑이나 장기의 경우는 경우의 수가 많기 때문에 게임트리를 전부 전개하는 것이 불가능하여 효율적인 탐색이 필요하다.** 

**⇒ 몬테카를로 트리탐색 ( MCTS ) 사용**  
-승패가 결정될때까지 무작위로 게임을 진행시키고 (플레이아웃 or 롤아웃) 이를 반복하여 승률을 통해 현재의 수가 얼마나 좋은지를 평가  

⑴ 알파고 

몬테카를로 트리탐색 +  심층강화학습  

알파고는 두가지 신경망을 사용한다. (value신경망과 policy 신경망) 
사람의 데이터로 먼저 두 신경망을 학습시킨 뒤 self-play로 대국을 반복해 경험데이터를 쌓고 학습을 강화 -> 이 부분이 강화학습    

에이전트의 복제체인 대전 상대가 환경의 역할을 하여 상호작용을 통해 승리 or 패배라는 보상을 얻는다. 

⑵ 알파고 제로  

**알파고 제로는 처음부터 학습데이터 없이 셀프 플레이로만 학습**  
알파고에서 사용했던 두가지 신경망을 하나의 신경망으로 표현하고 

보상함수 등은 코드로 주어지기 때문에 보드게임의 규칙을 알고 시작하긴 하지만 전략이나 수법을 학습하지 않는다는 의미에서 제로 라고 표현을 하고, 알파고 제로를 통해서 인간의 실력을 넘어서는 단계까지 스스로 학습할 수 있다는 가능성이 드러났다.  

⑶ 알파제로 

알파고 제로의 미세 조정 버전으로 바둑 이외에 체스 등의 전반적인 보드게임도 학습이 가능한 범용 알고리즘  

- 로봇 제어 

- NAS  

딥러닝의 신경망구조는 사람이 직접 설계하지만, NAS (Neural Architecture Search) AI가 스스로 구조를 설계해서 가장 성능이 좋은 모델을 찾는 방법  

- 건물 에너지 관리  

- 반도체 칩 설계  

### 5. 심층강화학습의 과제와 가능성  

- 오프라인 강화학습  

기존의 강화학습은 에이전트와 환경의 상호작용으로 경험을 쌓았다면 오프라인 강화학습은 이미 축적되어있는, 수집된 데이터셋을 가지고 학습한다.  
따라서 행동 정책과 대상 정책이 따로 있는 off-policy 기법으로 오프라인 강화학습 구현이 가능하다.  

- 모방 학습  

인간의 시범을 AI가 따라하며 배우는 학습 방법으로 **보상이 필요 없다**  

ex ) 요리 

1. 초기에 모방학습을 통해 행동 경로를 복사  
2. 실제 상황에서의 문제들을 대응하기 위해 강화학습으로 보정  

### 6. MDP 공식화  

강화학습의 이론은 MDP를 전제로 한다. MDP는 환경과 에이전트가 ( 상태, 행동, 보상 ) 을 서로 리턴하는 구조로 상태와 행동 그리고 보상을 바꿀 수 있기 때문에 MDP의 적용 범위는 다양하다.  

[ MDP 설정에 필요한 사항 ]

- 해결하고자 하는 과제는?  
    → 일회성 or 반복적/지속적?  
        - 일회성: Bandit 문제 가능성  
        - 반복적: MDP 구조 확정 → 강화학습 사용  

- 보상의 가치는?  
    → 즉시 보상에 집중? or 장기 보상 고려?  
        - 즉시만 고려: Myopic 정책 (Greedy)  
        - 장기 고려: Discounted reward → 강화학습 적용 적합  

- 에이전트의 행동 공간은?  
    → 이산적 or 연속적?  
        - 이산: Q-learning, SARSA, DQN  
        - 연속: Policy Gradient, DDPG, PPO 등  

- 환경 상태는 완전 관측 가능한가?  
    → 완전 관측(MDP) or 부분 관측(POMDP)?  
        - MDP: 일반 알고리즘 적용  
        - POMDP: RNN 기반 A3C, R2D2, Recurrent Q-learning  

- 환경이 확률적이거나 모델이 없는가?  
    → 모델 있음(Model-based) or 없음(Model-free)?  
        - Model-based: Dyna-Q, MCTS  
        - Model-free: 대부분의 DRL 알고리즘 (DQN, PPO 등)  

- 수익의 할인율 γ는 얼마인가?  
    → γ=0: 즉시 보상만, γ→1: 미래 보상도 중시  
        - γ=0: Bandit problem 적합  
        - 0 < γ < 1: 일반적인 강화학습 환경  

- 에이전트와 환경의 경계는?  
    → 제어 가능한 건 모두 에이전트  
        - 중요한 건 **관찰-행동 루프를 설계하는 방식**


---

**최적에 도달했다고 판단할 수 있는 보상함수의 기준을 정할 수 있다면 강화학습이 가능하다.**

---  

인간의 지혜는 경험이 세대를 통해 전승되어왔지만, AI는 수많은 시뮬레이션을 통해 그 오랜 시간을 집약적으로 응축한 결과물을 도출한다.  

이학계열에서는 결과가 왜 나왔는지를 탐구하기 때문에 가설을 세우고 이를 증명하기 위해 시간을 투자한다. 각 실험의 결과물이 결국 가설을 뒷받침하기 위해 필요하다.  
하지만 예술계열에서는 결과물 자체를 도출하기 위해 시간을 투자한다.  
이러한 차이점으로 이학계열에서는 AI가 도구로 이용되지만 예술계열에서는 생산자 그 자체로 역할을 하여 경쟁자가 된다.  

건축에서 공간의 역사성, 사회성을 해석하고 건축이 왜 이렇게 지어졌는지에 대한 서사를 부여하는 것은 건축가의 몫  
그 외에 시뮬레이션이나 다양한 선택지를 제공하는 도구로 AI를 사용  